#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
OKXå†å²æ•°æ®ä¸‹è½½å·¥å…·

åŠŸèƒ½ï¼š
1. ä¸‹è½½OKXå†å²Kçº¿æ•°æ®ï¼ˆæœ€å¤š10å¹´ï¼‰
2. æ”¯æŒå¤šç§æ—¶é—´å‘¨æœŸï¼ˆ1m, 5m, 15m, 1h, 4h, 1dï¼‰
3. æ–­ç‚¹ç»­ä¼ 
4. è¿›åº¦æ˜¾ç¤º
5. ä¿å­˜ä¸ºCSVå’ŒParquetæ ¼å¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python download_okx_data.py --symbol BTC-USDT --period 1h --years 3
"""

import sys
sys.path.insert(0, '..')

import requests
import pandas as pd
import time
from datetime import datetime, timedelta
from pathlib import Path
import argparse
import logging
from typing import Optional, List
import json

logging.basicConfig(
    level=logging.INFO,  # æ­£å¸¸æ¨¡å¼
    format='[%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)


class OKXDataDownloader:
    """OKXæ•°æ®ä¸‹è½½å™¨"""
    
    BASE_URL = "https://www.okx.com"
    API_ENDPOINT = "/api/v5/market/candles"
    
    # OKX APIé™åˆ¶ï¼šæ¯æ¬¡æœ€å¤š300æ¡
    MAX_CANDLES_PER_REQUEST = 300
    
    # æ—¶é—´å‘¨æœŸæ˜ å°„ï¼ˆç§’ï¼‰
    PERIOD_SECONDS = {
        '1m': 60,
        '5m': 300,
        '15m': 900,
        '30m': 1800,
        '1h': 3600,
        '2h': 7200,
        '4h': 14400,
        '1d': 86400,
        '1w': 604800,
    }
    
    # OKX APIæ—¶é—´å‘¨æœŸæ ¼å¼æ˜ å°„ï¼ˆéœ€è¦å¤§å†™ï¼‰
    PERIOD_TO_OKX = {
        '1m': '1m',
        '5m': '5m',
        '15m': '15m',
        '30m': '30m',
        '1h': '1H',   # éœ€è¦å¤§å†™
        '2h': '2H',   # éœ€è¦å¤§å†™
        '4h': '4H',   # éœ€è¦å¤§å†™
        '1d': '1D',   # éœ€è¦å¤§å†™
        '1w': '1W',   # éœ€è¦å¤§å†™
    }
    
    def __init__(self, data_dir: str = "data/okx"):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            data_dir: æ•°æ®ä¿å­˜ç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"æ•°æ®ä¿å­˜ç›®å½•: {self.data_dir.absolute()}")
    
    def download_historical_data(
        self,
        symbol: str = "BTC-USDT",
        period: str = "1h",
        years: int = 3,
        force_redownload: bool = False
    ) -> pd.DataFrame:
        """
        ä¸‹è½½å†å²æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹ï¼ˆå¦‚BTC-USDTï¼‰
            period: æ—¶é—´å‘¨æœŸï¼ˆ1m, 5m, 15m, 1h, 4h, 1dç­‰ï¼‰
            years: ä¸‹è½½å¹´æ•°ï¼ˆæœ€å¤š10å¹´ï¼‰
            force_redownload: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
        
        Returns:
            pd.DataFrame: å†å²æ•°æ®
        """
        logger.info("="*80)
        logger.info(f"å¼€å§‹ä¸‹è½½ {symbol} {period} Kçº¿æ•°æ®ï¼ˆæœ€è¿‘{years}å¹´ï¼‰")
        logger.info("="*80)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        csv_file = self.data_dir / f"{symbol.replace('-', '_')}_{period}_{years}y.csv"
        parquet_file = self.data_dir / f"{symbol.replace('-', '_')}_{period}_{years}y.parquet"
        
        if not force_redownload and csv_file.exists():
            logger.info(f"æ•°æ®æ–‡ä»¶å·²å­˜åœ¨: {csv_file}")
            logger.info("ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶é‡æ–°ä¸‹è½½")
            df = pd.read_csv(csv_file)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            logger.info(f"è¯»å–å®Œæˆ: {len(df)}æ¡æ•°æ®")
            return df
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        end_time = datetime.now()
        start_time = end_time - timedelta(days=365 * years)
        
        logger.info(f"æ—¶é—´èŒƒå›´: {start_time} è‡³ {end_time}")
        
        # ä¼°ç®—éœ€è¦çš„è¯·æ±‚æ¬¡æ•°
        period_seconds = self.PERIOD_SECONDS[period]
        total_seconds = (end_time - start_time).total_seconds()
        estimated_candles = int(total_seconds / period_seconds)
        estimated_requests = (estimated_candles // self.MAX_CANDLES_PER_REQUEST) + 1
        
        logger.info(f"é¢„è®¡Kçº¿æ•°: {estimated_candles:,}")
        logger.info(f"é¢„è®¡è¯·æ±‚æ•°: {estimated_requests:,}")
        logger.info(f"é¢„è®¡è€—æ—¶: {estimated_requests * 0.5:.1f}ç§’ï¼ˆ0.5ç§’/è¯·æ±‚ï¼‰")
        
        # åˆ†æ‰¹ä¸‹è½½
        all_data = []
        current_after = None  # ç¬¬ä¸€æ¬¡ä¸ä¼ afterï¼Œè·å–æœ€æ–°æ•°æ®
        request_count = 0
        max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        max_requests = estimated_requests * 3  # æœ€å¤§è¯·æ±‚æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        
        while True:
            # é‡è¯•æœºåˆ¶
            for retry in range(max_retries):
                try:
                    # è°ƒç”¨APIï¼ˆä½¿ç”¨afterå‚æ•°æŸ¥è¯¢å†å²æ•°æ®ï¼‰
                    data = self._fetch_candles(symbol, period, after=current_after)
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                except Exception as e:
                    if retry < max_retries - 1:
                        logger.warning(f"è¯·æ±‚å¤±è´¥ï¼Œ{3-retry}ç§’åé‡è¯•... ({retry+1}/{max_retries})")
                        time.sleep(3)
                    else:
                        raise  # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            
            try:
                
                if not data:
                    logger.info(f"æ²¡æœ‰æ›´å¤šæ•°æ® (è¯·æ±‚#{request_count + 1}, after={current_after})")
                    break
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®é¡ºåº
                if request_count == 0:
                    first_ts = datetime.fromtimestamp(int(data[0][0]) / 1000)
                    last_ts = datetime.fromtimestamp(int(data[-1][0]) / 1000)
                    logger.info(f"ğŸ“Š æ•°æ®é¡ºåºæ£€æŸ¥: ç¬¬ä¸€æ¡={first_ts} | æœ€åä¸€æ¡={last_ts}")
                    logger.info(f"   {'âœ… å€’åº(æ–°â†’æ—§)' if first_ts > last_ts else 'âŒ æ­£åº(æ—§â†’æ–°)'}")
                
                logger.debug(f"è·å–åˆ° {len(data)} æ¡æ•°æ® (è¯·æ±‚#{request_count + 1})")
                
                all_data.extend(data)
                request_count += 1
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¯·æ±‚æ¬¡æ•°
                if request_count >= max_requests:
                    logger.warning(f"âš ï¸  è¾¾åˆ°æœ€å¤§è¯·æ±‚æ¬¡æ•°é™åˆ¶({max_requests})ï¼Œåœæ­¢ä¸‹è½½")
                    logger.warning(f"   è¿™å¯èƒ½è¡¨æ˜æ—¶é—´æ¨è¿›æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
                    break
                
                # æ›´æ–°è¿›åº¦
                if request_count % 10 == 0:
                    latest_dt = datetime.fromtimestamp(int(data[0][0]) / 1000)
                    earliest_dt_temp = datetime.fromtimestamp(int(data[-1][0]) / 1000)
                    logger.info(f"å·²ä¸‹è½½: {len(all_data):,}æ¡æ•°æ® ({request_count}/{estimated_requests}è¯·æ±‚) | æ—¶é—´èŒƒå›´: {earliest_dt_temp} ~ {latest_dt}")
                
                # è·å–æœ€æ—©çš„æ—¶é—´æˆ³
                earliest_ts = int(data[-1][0])  # OKXè¿”å›çš„æ•°æ®æ˜¯å€’åºçš„
                earliest_dt = datetime.fromtimestamp(earliest_ts / 1000)
                
                # è·å–æœ€æ–°çš„æ—¶é—´æˆ³ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                latest_ts = int(data[0][0])
                latest_dt = datetime.fromtimestamp(latest_ts / 1000)
                
                # è°ƒè¯•ï¼šæ˜¾ç¤ºæ—¶é—´æ¨è¿›
                if request_count <= 3:  # å‰3æ¬¡è¯·æ±‚è¯¦ç»†è¾“å‡º
                    logger.info(f"ğŸ” è¯·æ±‚#{request_count} è¿”å›æ•°æ®:")
                    logger.info(f"   æ•°é‡: {len(data)}")
                    logger.info(f"   æ—¶é—´èŒƒå›´: {earliest_dt} ~ {latest_dt}")
                    logger.info(f"   æœ€æ—§æ—¶é—´æˆ³: {earliest_ts}")
                    logger.info(f"   ä¸‹æ¬¡after: {earliest_ts}")
                
                # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°èµ·å§‹æ—¶é—´
                if earliest_dt <= start_time:
                    logger.info(f"å·²è¾¾åˆ°èµ·å§‹æ—¶é—´: {earliest_dt}")
                    break
                
                # æ›´æ–°ä¸‹ä¸€æ¬¡è¯·æ±‚çš„afterå‚æ•°ï¼ˆä½¿ç”¨æœ€æ—§æ—¶é—´æˆ³è·å–æ›´æ—©çš„æ•°æ®ï¼‰
                current_after = earliest_ts
                
                # é¿å…è¯·æ±‚è¿‡å¿«ï¼ˆOKXé™åˆ¶ï¼š20æ¬¡/2ç§’ï¼‰
                time.sleep(0.1)  # åŠ å¿«é€Ÿåº¦ï¼š0.1ç§’/è¯·æ±‚
                
            except Exception as e:
                logger.error(f"ä¸‹è½½å‡ºé”™: {e}")
                logger.info(f"å·²ä¸‹è½½{len(all_data)}æ¡æ•°æ®ï¼Œå°è¯•ä¿å­˜...")
                break
        
        # è½¬æ¢ä¸ºDataFrame
        df = self._convert_to_dataframe(all_data)
        
        # å»é‡ï¼ˆæŒ‰æ—¶é—´æˆ³ï¼‰
        df = df.drop_duplicates(subset=['timestamp'], keep='first')
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´
        df = df[df['timestamp'] >= start_time]
        
        # æ’åºï¼ˆæŒ‰æ—¶é—´å‡åºï¼‰
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        logger.info(f"\nâœ… ä¸‹è½½å®Œæˆ: {len(df):,}æ¡æ•°æ®")
        logger.info(f"   æ—¶é—´èŒƒå›´: {df['timestamp'].min()} è‡³ {df['timestamp'].max()}")
        logger.info(f"   ä»·æ ¼èŒƒå›´: ${df['low'].min():.2f} - ${df['high'].max():.2f}")
        
        # ä¿å­˜æ•°æ®
        self._save_data(df, csv_file, parquet_file)
        
        return df
    
    def _fetch_candles(
        self,
        symbol: str,
        period: str,
        after: Optional[int] = None
    ) -> List:
        """
        è°ƒç”¨OKX APIè·å–Kçº¿æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹
            period: æ—¶é—´å‘¨æœŸ
            after: ç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰- æŸ¥è¯¢æ­¤æ—¶é—´ä¹‹åï¼ˆæ›´æ—©ï¼‰çš„æ•°æ®
        
        Returns:
            List: Kçº¿æ•°æ®
        """
        url = f"{self.BASE_URL}{self.API_ENDPOINT}"
        
        # è½¬æ¢ä¸ºOKXæ ¼å¼ï¼ˆå¤§å†™ï¼‰
        okx_period = self.PERIOD_TO_OKX.get(period, period)
        
        params = {
            'instId': symbol,
            'bar': okx_period,  # ä½¿ç”¨OKXæ ¼å¼
            'limit': self.MAX_CANDLES_PER_REQUEST,
        }
        
        if after:
            params['after'] = after
        
        logger.debug(f"APIè¯·æ±‚: {url} | params={params}")
        
        response = requests.get(url, params=params, timeout=30)  # å¢åŠ è¶…æ—¶æ—¶é—´
        response.raise_for_status()
        
        result = response.json()
        
        if result['code'] != '0':
            raise Exception(f"APIé”™è¯¯: {result.get('msg', 'Unknown error')}")
        
        logger.debug(f"APIè¿”å›: {len(result.get('data', []))} æ¡æ•°æ®")
        
        return result['data']
    
    def _convert_to_dataframe(self, data: List) -> pd.DataFrame:
        """
        è½¬æ¢ä¸ºDataFrame
        
        OKXè¿”å›æ ¼å¼ï¼š
        [timestamp, open, high, low, close, volume, volumeCcy, volumeCcyQuote, confirm]
        
        Args:
            data: åŸå§‹æ•°æ®
        
        Returns:
            pd.DataFrame: æ ¼å¼åŒ–çš„æ•°æ®
        """
        df = pd.DataFrame(data, columns=[
            'timestamp', 'open', 'high', 'low', 'close',
            'volume', 'volume_ccy', 'volume_quote', 'confirm'
        ])
        
        # è½¬æ¢æ•°æ®ç±»å‹
        df['timestamp'] = pd.to_datetime(df['timestamp'].astype(int), unit='ms')
        df['open'] = df['open'].astype(float)
        df['high'] = df['high'].astype(float)
        df['low'] = df['low'].astype(float)
        df['close'] = df['close'].astype(float)
        df['volume'] = df['volume'].astype(float)
        df['volume_quote'] = df['volume_quote'].astype(float)
        
        # åˆ é™¤ä¸éœ€è¦çš„åˆ—
        df = df.drop(['volume_ccy', 'confirm'], axis=1)
        
        return df
    
    def _save_data(
        self,
        df: pd.DataFrame,
        csv_file: Path,
        parquet_file: Path
    ):
        """
        ä¿å­˜æ•°æ®
        
        Args:
            df: æ•°æ®
            csv_file: CSVæ–‡ä»¶è·¯å¾„
            parquet_file: Parquetæ–‡ä»¶è·¯å¾„
        """
        # ä¿å­˜CSVï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
        df.to_csv(csv_file, index=False)
        logger.info(f"\nğŸ’¾ CSVå·²ä¿å­˜: {csv_file}")
        logger.info(f"   æ–‡ä»¶å¤§å°: {csv_file.stat().st_size / 1024 / 1024:.2f} MB")
        
        # ä¿å­˜Parquetï¼ˆèŠ‚çœç©ºé—´ï¼Œè¯»å–å¿«ï¼‰- å¯é€‰
        try:
            df.to_parquet(parquet_file, index=False)
            logger.info(f"\nğŸ’¾ Parquetå·²ä¿å­˜: {parquet_file}")
            logger.info(f"   æ–‡ä»¶å¤§å°: {parquet_file.stat().st_size / 1024 / 1024:.2f} MB")
        except Exception as e:
            logger.warning(f"\nâš ï¸  Parquetä¿å­˜å¤±è´¥: {e}")
            logger.warning(f"   æç¤º: pip install pyarrow")
            logger.info(f"   CSVæ–‡ä»¶å·²ä¿å­˜ï¼Œå¯æ­£å¸¸ä½¿ç”¨")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'symbol': csv_file.stem.rsplit('_', 2)[0],
            'period': csv_file.stem.rsplit('_', 2)[1],
            'start_time': df['timestamp'].min().isoformat(),
            'end_time': df['timestamp'].max().isoformat(),
            'num_candles': len(df),
            'price_range': {
                'min': float(df['low'].min()),
                'max': float(df['high'].max()),
            },
            'download_time': datetime.now().isoformat(),
        }
        
        metadata_file = csv_file.with_suffix('.json')
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"ğŸ“‹ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='OKXå†å²æ•°æ®ä¸‹è½½å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # ä¸‹è½½BTC-USDTæœ€è¿‘3å¹´çš„1å°æ—¶Kçº¿
  python download_okx_data.py --symbol BTC-USDT --period 1h --years 3
  
  # ä¸‹è½½ETH-USDTæœ€è¿‘1å¹´çš„15åˆ†é’ŸKçº¿
  python download_okx_data.py --symbol ETH-USDT --period 15m --years 1
  
  # å¼ºåˆ¶é‡æ–°ä¸‹è½½
  python download_okx_data.py --symbol BTC-USDT --period 1h --years 3 --force

æ”¯æŒçš„æ—¶é—´å‘¨æœŸï¼š
  1m, 5m, 15m, 30m, 1h, 2h, 4h, 1d, 1w
        """
    )
    
    parser.add_argument(
        '--symbol',
        default='BTC-USDT',
        help='äº¤æ˜“å¯¹ï¼ˆé»˜è®¤ï¼šBTC-USDTï¼‰'
    )
    
    parser.add_argument(
        '--period',
        default='1h',
        choices=['1m', '5m', '15m', '30m', '1h', '2h', '4h', '1d', '1w'],
        help='æ—¶é—´å‘¨æœŸï¼ˆé»˜è®¤ï¼š1hï¼‰'
    )
    
    parser.add_argument(
        '--years',
        type=int,
        default=3,
        help='ä¸‹è½½å¹´æ•°ï¼ˆé»˜è®¤ï¼š3å¹´ï¼Œæœ€å¤š10å¹´ï¼‰'
    )
    
    parser.add_argument(
        '--data-dir',
        default='../data/okx',
        help='æ•°æ®ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ï¼š../data/okxï¼‰'
    )
    
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼ˆè¦†ç›–å·²å­˜åœ¨çš„æ•°æ®ï¼‰'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = OKXDataDownloader(data_dir=args.data_dir)
    
    # ä¸‹è½½æ•°æ®
    df = downloader.download_historical_data(
        symbol=args.symbol,
        period=args.period,
        years=min(args.years, 10),  # æœ€å¤š10å¹´
        force_redownload=args.force
    )
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*80)
    print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
    print("="*80)
    print(f"\næ•°æ®é¢„è§ˆï¼ˆå‰5æ¡ï¼‰:")
    print(df.head())
    print(f"\nåŸºæœ¬ç»Ÿè®¡:")
    print(df.describe())
    
    print("\n" + "="*80)
    print("âœ… å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°ï¼Œå¯ç”¨äºåç»­æµ‹è¯•å’Œå›æµ‹")
    print("="*80)


if __name__ == "__main__":
    main()

